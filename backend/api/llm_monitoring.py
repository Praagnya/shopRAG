import time
import json
import logging
import os
from typing import Any, Dict

# Ensure logs directory exists
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

LOG_FILE = os.path.join(LOG_DIR, "llm_logs.json")

# Create logger
logger = logging.getLogger("llm_monitor")
logger.setLevel(logging.INFO)

handler = logging.FileHandler(LOG_FILE)
handler.setFormatter(logging.Formatter("%(message)s"))
logger.addHandler(handler)


def log_llm_event(event: Dict[str, Any]):
    """Write a monitoring event to JSON logs."""
    logger.info(json.dumps(event))


def monitor_llm_call(model: str, prompt: str, fn):
    """
    Wraps an LLM call and returns BOTH:
      (response, metrics_dict)

    metrics_dict contains:
      - prompt_tokens
      - completion_tokens
      - total_tokens
      - latency_ms
      - error
      - prompt_chars
      - response_chars
    """
    start = time.time()
    error = None
    response_text = ""
    usage = {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0,
    }

    try:
        # Actual OpenAI call
        response = fn()

        response_text = response.choices[0].message.content

        if hasattr(response, "usage"):
            usage["prompt_tokens"] = getattr(response.usage, "prompt_tokens", 0)
            usage["completion_tokens"] = getattr(response.usage, "completion_tokens", 0)
            usage["total_tokens"] = getattr(response.usage, "total_tokens", 0)

    except Exception as e:
        error = str(e)
        response = None

    latency_ms = round((time.time() - start) * 1000, 2)

    # JSON LOG EVENT
    event = {
        "timestamp": time.time(),
        "model": model,
        "prompt_chars": len(prompt),
        "response_chars": len(response_text),
        "latency_ms": latency_ms,
        "prompt_tokens": usage["prompt_tokens"],
        "completion_tokens": usage["completion_tokens"],
        "total_tokens": usage["total_tokens"],
        "error": error,
    }

    log_llm_event(event)

    # Return detailed info to LLMClient so it can update Prometheus
    info = {
        "prompt_tokens": usage["prompt_tokens"],
        "completion_tokens": usage["completion_tokens"],
        "total_tokens": usage["total_tokens"],
        "latency_ms": latency_ms,
        "error": error,
    }

    if error:
        raise Exception(f"LLM Error: {error}")

    return response, info


# ===============================================================
# Prometheus Metrics for Version B (Full RAG)
# ===============================================================
from prometheus_client import Counter, Histogram

llm_calls_total = Counter(
    "llm_calls_total", "Total number of LLM calls"
)

llm_prompt_tokens_total = Counter(
    "llm_prompt_tokens_total", "Total number of prompt tokens sent to LLM"
)

llm_completion_tokens_total = Counter(
    "llm_completion_tokens_total", "Total number of completion tokens generated by LLM"
)

llm_latency_ms = Histogram(
    "llm_latency_ms", "LLM response latency in milliseconds"
)

llm_cost_usd_total = Counter(
    "llm_cost_usd_total", "Estimated USD cost of LLM usage"
)

# Cost model (adjustable)
COST_PER_1K_IN = 0.00015
COST_PER_1K_OUT = 0.00060


def record_llm_metrics(prompt_tokens: int, completion_tokens: int, latency_ms: float):
    """Record Prometheus metrics for Version B RAG."""
    llm_calls_total.inc()
    llm_prompt_tokens_total.inc(prompt_tokens)
    llm_completion_tokens_total.inc(completion_tokens)
    llm_latency_ms.observe(latency_ms)

    cost = (prompt_tokens / 1000) * COST_PER_1K_IN + (completion_tokens / 1000) * COST_PER_1K_OUT
    llm_cost_usd_total.inc(cost)
