import time
import json
import logging
import os
from typing import Any, Dict
from backend.services.prom_metrics import (
    llm_prompt_chars_total,
    llm_response_chars_total,
    llm_latency_ms,
    llm_tokens_used_total,
    llm_errors_total
)

# Ensure logs directory exists
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

LOG_FILE = os.path.join(LOG_DIR, "llm_logs.json")

# Create logger
logger = logging.getLogger("llm_monitor")
logger.setLevel(logging.INFO)

handler = logging.FileHandler(LOG_FILE)
handler.setFormatter(logging.Formatter("%(message)s"))
logger.addHandler(handler)


def log_llm_event(event: Dict[str, Any]):
    """Write a monitoring event to JSON logs."""
    logger.info(json.dumps(event))

def monitor_llm_call(model, prompt, fn):

    start = time.time()
    try:
        response = fn()
        elapsed = (time.time() - start) * 1000

        # Extract token & char data
        usage = response.usage
        prompt_chars = len(prompt)
        response_chars = len(response.choices[0].message.content)

        llm_prompt_chars_total.inc(prompt_chars)
        llm_response_chars_total.inc(response_chars)
        llm_tokens_used_total.inc(usage.total_tokens)
        llm_latency_ms.observe(elapsed)

        response.llm_latency_ms = elapsed
        return response

    except Exception:
        llm_errors_total.inc()
        raise

# ===============================================================
# Prometheus Metrics for Version B (Full RAG)
# ===============================================================
from prometheus_client import Counter, Histogram

llm_calls_total = Counter(
    "llm_calls_total", "Total number of LLM calls"
)

llm_prompt_tokens_total = Counter(
    "llm_prompt_tokens_total", "Total number of prompt tokens sent to LLM"
)

llm_completion_tokens_total = Counter(
    "llm_completion_tokens_total", "Total number of completion tokens generated by LLM"
)

llm_latency_ms = Histogram(
    "llm_latency_ms", "LLM response latency in milliseconds"
)

llm_cost_usd_total = Counter(
    "llm_cost_usd_total", "Estimated USD cost of LLM usage"
)

# Cost model (adjustable)
COST_PER_1K_IN = 0.00015
COST_PER_1K_OUT = 0.00060


def record_llm_metrics(prompt_tokens: int, completion_tokens: int, latency_ms: float):
    """Record Prometheus metrics for Version B RAG."""
    llm_calls_total.inc()
    llm_prompt_tokens_total.inc(prompt_tokens)
    llm_completion_tokens_total.inc(completion_tokens)
    llm_latency_ms.observe(latency_ms)

    cost = (prompt_tokens / 1000) * COST_PER_1K_IN + (completion_tokens / 1000) * COST_PER_1K_OUT
    llm_cost_usd_total.inc(cost)
